# Thematic Analysis: Mentoring Onboarding

**Task Category:** Meta Work\n**Task Name:** Mentoring Onboarding\n\n**Generated:** 2025-10-01 00:14:26\n**Number of Participants:** 23\n**Data Source:** `data-meta_work-mentoring_onboarding.csv`\n\n---\n\n# Thematic Analysis — Mentoring Onboarding (Meta Work)

This report synthesizes manually coded survey responses from software developers about AI usage in Mentoring Onboarding (a subdomain of Meta Work). I preserve the research team’s themes, codes, and descriptions and integrate participant quotes to produce coherent narratives, analyze tensions, and draw implications for AI tool design. Counts of participants per theme are computed from unique PID values in the coded quotes and rounded to the nearest five where indicated.

---

## 1) Core Themes

### A. Where AI is Wanted

#### Theme: AI-assisted onboarding support/walkthroughs

Participants consistently want AI to play an active, practical role in onboarding by generating learning plans, maintaining and surfacing documentation, and answering newcomer questions. The researcher-coded description—“Participants want AI to create onboarding/training plans, provide walkthroughs for new joiners, and answer newcomer questions. AI is envisioned as actively guiding new employees through processes or knowledge areas.”—captures this orientation: AI is seen as a scalable way to reduce the cognitive and administrative load of bringing new joiners up to speed. Several respondents explicitly framed desired outcomes as time savings and knowledge transfer: “Assist in onboarding, documentation and learning new technologies.” (Participant 157) and “Auto-generated training plan for new onboardings.” (Participant 272).

Participants also imagine AI going beyond static help to act as a proactive guide or “lead” in some situations: “Mentoring and onboarding automated by AI would be great. Rather than being just an assistant, it should also take the lead role sometimes.” (Participant 318). At the same time, many set boundaries focused on task type: AI is expected to handle routinized, searchable, and boilerplate tasks—building training outlines, surfacing resources, transcribing meetings, and drafting documentation—rather than replacing human relational work (this boundary is explored in the “Where AI is Not Wanted” themes). For example, people want AI to help “search/finding resources; communication: transcription, summaries; documentation: boilerplate” (Participant 385) and to “help me brain dump the knowledge I've gained over the years to help customers and onboard new people.” (Participant 401).

Sub-themes identified:
- **AI-assisted onboarding support/walkthroughs**: Participants want AI to create onboarding/training plans, provide walkthroughs for new joiners, and answer newcomer questions. AI is envisioned as actively guiding new employees through processes or knowledge areas.

Number of participants: ~10 participants (count unique PIDs, round to nearest 5)

Representative quotes:
- *"Assist in onboarding, documentation and learning new technologies."* (Participant 157)
- *"I would like AI to help in onboarding and in learning new technologies. It has the ability to create learning plans which are very helpful."* (Participant 269)
- *"Mentoring and onboarding automated by AI would be great. Rather than being just an assistant, it should also take the lead role sometimes."* (Participant 318)

Confidence: High

---

### B. Where AI is Not Wanted

#### Theme: Human-centric and relational nature of mentoring

The core argument in this theme is that mentoring and onboarding are fundamentally human, relationship-driven activities that cannot be fully delegated to AI. The researcher-coded description—“Mentoring is fundamentally about human-to-human interaction, relationship building, and integration into teams. Participants stress its cultural, interpersonal, and relational essence, which AI cannot replicate.”—is echoed strongly in responses. Participants emphasize integration into social teams, building trust, and the mentor’s role in demonstrating norms and tacit knowledge: “Mentoring is a human activity and would not be helpful for AI to do...this is a human-to-human activity to get the person engaged and integrated with other humans.” (Participant 27). Several respondents allowed for limited AI support for rote tasks but were explicit that the relational core must remain human: “Using AI for helping new people with the rote steps of onboarding ... is fine, but there needs to be a human connection.” (Participant 70).

Concerns include loss of personal connection, reduced opportunities for mentors to form bonds with new joiners, and undermining cultural transmission. Participants argued that handing off mentoring to AI would deprive both mentee and mentor of developmental value: “Onboarding, because new team members need to interact with their team members to build relationships. That's an essential part of being human that I don't believe AI can replace in a satisfactory way for many many years to come.” (Participant 122).

Sub-themes identified:
- **Human-centric and relational nature of mentoring**: Mentoring is fundamentally about human-to-human interaction, relationship building, and integration into teams. Participants stress its cultural, interpersonal, and relational essence, which AI cannot replicate.

Number of participants: ~5 participants (count unique PIDs, round to nearest 5)

Representative quotes:
- *"Mentoring is a human activity and would not be helpful for AI to do...this is a human-to-human activity to get the person engaged and integrated with other humans."* (Participant 27)
- *"Using AI for helping new people with the rote steps of onboarding ... is fine, but there needs to be a human connection."* (Participant 70)
- *"Onboarding, because new team members need to interact with their team members to build relationships."* (Participant 122)

Confidence: Medium

---

#### Theme: Need for oversight (AI as assistant, not replacement)

Participants frequently qualified support for AI by insisting on human oversight: AI can help with documentation or routine onboarding steps, but humans must remain the primary agents and verifiers. The code description—“AI might assist with rote onboarding steps, documentation, or simple tasks, but human oversight and connection are essential.”—matches comments warning about hallucinations and incorrect guidance that could mislead new hires. “AI should definitely not be the primary factor in documentation, communication, or mentoring/onboarding. Its tendency to hallucinate is too great to risk in these areas without significant human oversight.” (Participant 17) captures this exact worry.

Some participants articulated a mixed model: AI provides scaffolding for routine tasks (e.g., entitlements, repository cloning) while mentors provide personal context and final verification: “Mentoring and onboarding is personal, but could be assisted by AI in some ways, but I wouldn't want it to do the mentoring directly.” (Participant 83). This theme therefore defines a clear boundary condition: AI = tool, humans = owners and validators.

Sub-themes identified:
- **Need for oversight (AI as assistant, not replacement)**: AI might assist with rote onboarding steps, documentation, or simple tasks, but human oversight and connection are essential.

Number of participants: ~5 participants (count unique PIDs, round to nearest 5)

Representative quotes:
- *"AI should definitely not be the primary factor in documentation, communication, or mentoring/onboarding. Its tendency to hallucinate is too great to risk in these areas without significant human oversight."* (Participant 17)
- *"Using AI for helping new people with the rote steps of onboarding ... is fine, but there needs to be a human connection."* (Participant 70)
- *"Mentoring and onboarding is personal, but could be assisted by AI in someways, but I wouldn't want it to do the mentoring directly."* (Participant 83)

Confidence: Low

---

#### Theme: Limitations in handling nuances (empathy, subjectivity, culture)

Respondents expressed skepticism about AI’s capacity to understand cultural context, exercise empathy, or handle subjective, opinion-based mentoring. The coded description—“Participants highlight AI’s inability to capture cultural nuances, empathy, or subjective, opinion-based aspects of mentoring.”—is reflected in direct statements that mentoring requires consolation, moral judgement, and culturally informed advice. “I don't want AI to handle mentoring. This is a very cultural task that requires human-human communication.” (Participant 10) and “Mentoring, as it's often opinion-based and varies by mentor/mentee.” (Participant 71) underscore the idea that mentoring is not a deterministic information transfer but an interpretive, situational practice.

This theme also links to concerns about intellectual ownership and the mentor’s role in personal development: participants worry AI cannot replicate how mentors learn through mentoring or how they model leadership behaviors for mentees. That subjective, developmental component was framed as both fragile and essential.

Sub-themes identified:
- **Limitations in handling nuances (empathy, subjectivity, culture)**: Participants highlight AI’s inability to capture cultural nuances, empathy, or subjective, opinion-based aspects of mentoring.

Number of participants: ~5 participants (count unique PIDs, round to nearest 5)

Representative quotes:
- *"I don't want AI to handle mentoring. This is a very cultural task that requires human-human communication."* (Participant 10)
- *"Mentoring, as it's often opinion-based and varies by mentor/mentee."* (Participant 71)
- *"I do not want AI to handle ... mentoring requires empathy. I think these are activities that humans need to do to grow themselves as well."* (Participant 228)

Confidence: Low

---

#### Theme: High risk of errors and negative outcomes

Some participants emphasized the potential harm of incorrect AI guidance during onboarding and mentorship. The code description—“Mistakes in mentoring or onboarding could cause serious harm. Participants fear AI hallucinations or misguidance could negatively impact mentees.”—is evident in concerns about the cost of errors: “I don't want it to do mentoring because the cost of it getting it wrong is terrible and it will get it wrong. The point of mentoring and onboarding is to teach people, and the value of getting it right is huge in the cost of getting it wrong is also huge.” (Participant 357). Others generalized mistrust in probabilistic models: “Filling out details of documentation (I don't trust a stochastic parrot to be capable of getting it right, no matter how refined); mentoring and onboarding (building the personal connection is the point!! (but there are places AI can help)).” (Participant 385).

These comments anchor a safety-centered design concern: onboarding errors can cascade into lost productivity, culture mismatch, or incorrect technical practices—high-cost harms that participants think AI alone is not ready to absorb.

Sub-themes identified:
- **High risk of errors and negative outcomes**: Mistakes in mentoring or onboarding could cause serious harm. Participants fear AI hallucinations or misguidance could negatively impact mentees.

Number of participants: ~0 participants (count unique PIDs, round to nearest 5)

Representative quotes:
- *"I don't want it to do mentoring because the cost of it getting it wrong is terrible and it will get it wrong."* (Participant 357)
- *"Filling out details of documentation (I don't trust a stochastic parrot to be capable of getting it right ...); mentoring and onboarding (building the personal connection is the point!!)."* (Participant 385)

Confidence: Low

---

#### Theme: Mentor’s growth and development

A distinct line of reasoning holds that mentoring benefits mentors as well as mentees; automating mentorship risks stripping mentors of developmental opportunities. The code description—“Mentoring benefits the mentor as much as the mentee. Offloading it to AI undermines human learning, empathy, and leadership development.”—was voiced directly: “I do not want AI to handle ... mentoring requires empathy. I think these are activities that humans need to do to grow themselves as well (e.g. mentoring someone teaches the mentor as well).” (Participant 228). This concern frames mentoring as a reciprocal practice and implies organizational loss if AI replaces that reciprocal interaction.

Sub-themes identified:
- **Mentor’s growth and development**: Mentoring benefits the mentor as much as the mentee. Offloading it to AI undermines human learning, empathy, and leadership development.

Number of participants: ~0 participants (count unique PIDs, round to nearest 5)

Representative quotes:
- *"Mentoring requires empathy. I think these are activities that humans need to do to grow themselves as well (e.g. mentoring someone teaches the mentor as well)."* (Participant 228)

Confidence: Low

---

## 2) Cross-Cutting Patterns

- Complementary desires and concerns: Developers want AI to handle routinized, administrative, and knowledge-management parts of onboarding (learning plans, documentation, search, summaries), while strongly resisting AI takeover of relational and subjective functions. This creates a clear complementarity: AI for scaffolding, humans for socialization and culture transfer.

- Conditional acceptance: Acceptance is explicitly conditional on scope and oversight. Many participants accept AI support for "rote steps" (entitlements, repo setup, boilerplate docs) but only under human supervision. The dominant boundary conditions are (a) human-in-the-loop verification, (b) limited autonomy for AI in judgement-sensitive contexts, and (c) preserving human-to-human relationship opportunities.

- Task-specific nuances: Mentoring onboarding differs from other software tasks because it involves cultural transmission, tacit knowledge, and developmental feedback loops. These characteristics make errors more consequential and subjective judgments central, reducing tolerance for opaque, fully automated solutions.

- Trust and control dynamics: Trust is fragile and must be engineered: participants emphasize predictable behavior, verifiable outputs, and explicit delineation of responsibilities. There is a strong preference for AI as an assistant whose outputs are traceable and editable by human mentors, rather than as an autonomous mentor.

## 3) Outliers and Edge Cases

- Minority perspectives that contradict dominant themes: A few participants proposed that AI could sometimes “take the lead” in onboarding (Participant 318). This minority view suggests appetite for more proactive AI when the cost of error is low and the process is highly standardized.

- Unique insights that don't fit neatly: Some answers combined distrust of AI generically with pragmatic willingness to use it for documentation and knowledge capture (e.g., Participant 401 wanting AI to “brain dump” accumulated knowledge). This sits between the polar positions and suggests value in AI-assisted knowledge elicitation tools even for those skeptical of AI mentors.

- Ambivalent responses: Several respondents simultaneously recommended AI for routine tasks and warned against its use in relational or judgment contexts (Participant 70 exemplifies this ambivalence). These mixed responses point to nuanced, context-dependent acceptance—people are willing to use AI when it reduces friction but not when it reduces human contact.

- Contradictions within individuals: Participant 385 appears in both “want” (help finding resources, boilerplate docs) and “don’t want” (distrust of stochastic models for documentation accuracy), reflecting conditional trust—useful for low-risk chores, unacceptable for accuracy-critical documentation.

## 4) Implications for AI Tool Design

High-level synthesis: Developers want AI tools that reliably automate repetitive onboarding tasks and surface curated knowledge, while explicitly preserving and supporting human-led relationship-building, oversight, and mentor development. Trust, explainability, and scoped autonomy are central to acceptance.

#### Key "Must Haves" (features designers should prioritize)

- **Onboarding plan generator and curator**
  - Capability: Create, customize, and update onboarding/training plans tied to role and team context; allow mentors to edit and approve generated plans.
  - Rationale: Multiple participants asked for auto-generated learning plans and walkthroughs (Participants 272, 269, 157).
  - Example: “Auto-generated training plan for new onboardings.” (Participant 272)

- **Doc + knowledge surfacing and synthesis**
  - Capability: Index team documentation, provide search/finding, create boilerplate drafts, and produce meeting transcripts/summaries with provenance links to source docs.
  - Rationale: Participants want help finding resources and drafting documentation; provenance enables verification.
  - Example: “Onboarding: search/finding resources; communication: transcription, summaries; documentation: boilerplate and search/finding resources.” (Participant 385)

- **Human-in-the-loop verification and edit controls**
  - Capability: Explicit review workflows, easy edit/approval mechanisms, and visible confidence or provenance metadata on AI suggestions.
  - Rationale: Overarching desire for oversight to prevent hallucinations and errors (Participant 17).
  - Example: “Its tendency to hallucinate is too great to risk ... without significant human oversight.” (Participant 17)

- **Scoped automation for routine administrative tasks**
  - Capability: Automate entitlements, repo cloning instructions, setup scripts, and checklist completion with clear handoffs to human mentors for context and relationship tasks.
  - Rationale: Participants accept AI for rote steps but not for relational mentoring (Participant 70).

#### Key "Must Not Haves" (design guardrails)

- **Autonomous mentoring without human oversight**
  - Risk: Loss of relational integration and potential for high-cost errors.
  - Rationale: Mentoring is human-centric and requires empathy and culture transfer (Participant 27).
  - Example: “Mentoring and onboarding shouldn't be done by an AI. It's a human thing that shouldn't be handed off to technology.” (Participant 98)

- **Opaque, unprovable recommendations**
  - Risk: Hallucinated or unattributed content leading to incorrect practices.
  - Rationale: Participants distrust stochastic outputs without provenance (Participant 385).
  - Example: “I don't trust a stochastic parrot to be capable of getting it right.” (Participant 385)

- **Replacing mentor development opportunities**
  - Risk: Organizational loss of leadership and learning pathways for mentors.
  - Rationale: Mentoring helps mentors grow; automating it undermines that growth (Participant 228).
  - Example: “Mentoring requires empathy. I think these are activities that humans need to do to grow themselves as well.” (Participant 228)

- **Handling culturally- or ethically-sensitive guidance**
  - Risk: Misguided or tone-deaf advice that harms social integration or trust.
  - Rationale: Mentoring is culturally situated and subjective (Participant 10).

#### Design Patterns to Resolve Tensions

- Human-in-the-loop augmentation: Provide AI-generated onboarding plans and resource summaries, but require mentor approval before those materials are delivered or presented to newcomers. This preserves efficiency while maintaining human authority.

- Clear scoping and role separation: Explicitly label AI outputs as “resource suggestions / drafts” vs. “mentor recommendations.” Separate UI flows for administrative automation (automatically run) and relational guidance (human-mediated).

- Explainability + provenance layer: Surface links to original documentation, confidence scores, and a changelog for AI-edited onboarding materials so mentors can quickly verify and adapt content.

- Mentor development affordances: Design tools that scaffold mentoring (e.g., suggested feedback templates, prompts to reflect) rather than replace it—preserving mentor growth while reducing friction.

---

## Executive Summary

- Developers want AI to automate routinized onboarding tasks—auto-generated learning plans, documentation drafting, resource search, transcripts, and setup checklists—so mentors can focus on people work.
- Developers do not want AI to replace the human, relational core of mentoring; building trust, culture, and empathy are seen as distinctly human responsibilities.
- Critical design implication: tools must be human-in-the-loop with clear provenance and approval workflows to prevent hallucinated or unverified guidance.
- Critical design implication: scope AI autonomy strictly—allow automation for administrative tasks but require human judgment for culturally or developmentally sensitive guidance.
- Notable tension: some participants favor proactive AI “leading” in highly standardized contexts, while many insist human oversight and relational presence remain primary.
- Recommendation: build assistive onboarding tools that generate drafts and surfacing features, expose provenance/confidence, and include easy mentor review/approval flows—design to augment mentors, not replace them.